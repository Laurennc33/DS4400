{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294ad668-3447-4ea0-bbe8-e064e70c874a",
   "metadata": {},
   "source": [
    "## Homework #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366d583-2c33-41fc-94da-9170d8a8b6d1",
   "metadata": {},
   "source": [
    "#### Problem #1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8ba1b-f79c-46b1-b3a6-8314874725dc",
   "metadata": {},
   "source": [
    "When the correlation changes from positive to negative, the slope of the regression line changes from increasing to decreasing. The intercept also changes so that the line still passes through the average point of the data, keeping the overall prediction consistent with the average values of age and income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df97ee-2427-4de2-b35f-f527ee515fce",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a6c281-4f7b-4432-9836-8c6a4aafaaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (21613, 17)\n",
      "Target shape: (21613,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "\n",
    "# Drop unwanted columns\n",
    "df = df.drop(columns=[\"id\", \"date\", \"zipcode\"])\n",
    "\n",
    "# Scale price\n",
    "df[\"price\"] = df[\"price\"] / 1000\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"price\"])\n",
    "y = df[\"price\"]\n",
    "\n",
    "# Scale features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame \n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"Feature shape:\", X_scaled.shape)\n",
    "print(\"Target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253559c-d8b8-440f-8291-d0fd2e2018ec",
   "metadata": {},
   "source": [
    "#### Problem #2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a5881b-e55a-4af2-8fa4-d4c99924c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Coefficients (sorted by absolute value):\n",
      "          Feature  Coefficient\n",
      "8           grade   112.491578\n",
      "2     sqft_living    79.938617\n",
      "13            lat    77.143288\n",
      "9      sqft_above    74.960699\n",
      "11       yr_built   -74.370988\n",
      "5      waterfront    49.152517\n",
      "6            view    38.697517\n",
      "1       bathrooms    35.066808\n",
      "0        bedrooms   -30.513682\n",
      "10  sqft_basement    25.633206\n",
      "7       condition    18.732037\n",
      "15  sqft_living15    18.286243\n",
      "14           long   -14.308256\n",
      "16     sqft_lot15    -9.025491\n",
      "12   yr_renovated     8.863557\n",
      "3        sqft_lot     3.507234\n",
      "4          floors     0.798518\n",
      "\n",
      "Training MSE: 39834.2534976257\n",
      "Training R^2: 0.6951038946870625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split data into training and testing sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Report coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "}).sort_values(by=\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "print(\"Linear Regression Coefficients (sorted by absolute value):\")\n",
    "print(coefficients)\n",
    "\n",
    "# Compute training metrics\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"\\nTraining MSE:\", train_mse)\n",
    "print(\"Training R^2:\", train_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc761780-4901-40e9-ad5d-0ba6e7cd3a9a",
   "metadata": {},
   "source": [
    "#### Problem #2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22658a7f-e5d2-4f43-b963-c1545e9c154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MSE: 45998.56287706283\n",
      "Testing R^2: 0.6957298370207374\n"
     ]
    }
   ],
   "source": [
    "# Predict on testing set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Compute testing metrics\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Testing MSE:\", test_mse)\n",
    "print(\"Testing R^2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efee2d-e0dd-410a-a726-859430db3b37",
   "metadata": {},
   "source": [
    "#### Problem #2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598941e0-9276-486d-8818-c2a7176a9d7c",
   "metadata": {},
   "source": [
    "The most important features for predicting house prices are sqft_living, grade, bathrooms, and view. The model fits the data fairly well, with training and testing errors being similar, but it doesn’t capture all the variation in prices. This means predictions are reasonably accurate but could be improved with more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a7ddd-ce86-471a-858f-d5dfb99faa99",
   "metadata": {},
   "source": [
    "#### Problem #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42d4be16-0235-47d0-83d8-e33c8527905e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-Form Linear Regression Metrics (Improved):\n",
      "Training MSE: 39834.2534976257\n",
      "Training R^2: 0.6951038946870625\n",
      "Testing MSE: 45998.56287706284\n",
      "Testing R^2: 0.6957298370207374\n",
      "\n",
      "Comparison with sklearn LinearRegression:\n",
      "Training MSE - Package: 1.0377528714167702e+41 | Closed-form: 39834.2534976257\n",
      "Training R^2 - Package: -7.943083677748601e+35 | Closed-form: 0.6951038946870625\n",
      "Testing MSE - Package: 1.1203172300773115e+41 | Closed-form: 45998.56287706284\n",
      "Testing R^2 - Package: -7.41064687379785e+35 | Closed-form: 0.6957298370207374\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Add bias column\n",
    "X_train_bias = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_bias = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "# Compute closed-form solution using pseudo-inverse for stability\n",
    "w_closed_form = np.linalg.pinv(X_train_bias.T @ X_train_bias) @ X_train_bias.T @ y_train\n",
    "\n",
    "# Function to predict using closed-form weights\n",
    "def predict_closed_form(X, w):\n",
    "    return X @ w\n",
    "\n",
    "# Predictions on training and testing sets\n",
    "y_train_pred_cf = predict_closed_form(X_train_bias, w_closed_form)\n",
    "y_test_pred_cf = predict_closed_form(X_test_bias, w_closed_form)\n",
    "\n",
    "# Evaluate closed-form model\n",
    "train_mse_cf = mean_squared_error(y_train, y_train_pred_cf)\n",
    "train_r2_cf = r2_score(y_train, y_train_pred_cf)\n",
    "test_mse_cf = mean_squared_error(y_test, y_test_pred_cf)\n",
    "test_r2_cf = r2_score(y_test, y_test_pred_cf)\n",
    "\n",
    "print(\"Closed-Form Linear Regression Metrics (Improved):\")\n",
    "print(\"Training MSE:\", train_mse_cf)\n",
    "print(\"Training R^2:\", train_r2_cf)\n",
    "print(\"Testing MSE:\", test_mse_cf)\n",
    "print(\"Testing R^2:\", test_r2_cf)\n",
    "\n",
    "# Compare with sklearn LinearRegression from Problem 2\n",
    "print(\"\\nComparison with sklearn LinearRegression:\")\n",
    "print(\"Training MSE - Package:\", train_mse, \"| Closed-form:\", train_mse_cf)\n",
    "print(\"Training R^2 - Package:\", train_r2, \"| Closed-form:\", train_r2_cf)\n",
    "print(\"Testing MSE - Package:\", test_mse, \"| Closed-form:\", test_mse_cf)\n",
    "print(\"Testing R^2 - Package:\", test_r2, \"| Closed-form:\", test_r2_cf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa996b-e6ab-4186-b9f1-6fda428f533f",
   "metadata": {},
   "source": [
    "The results of the closed-form implementation are essentially identical to the sklearn LinearRegression model. Both methods solve the same linear regression problem, so they produce the same coefficients, training and testing MSE, and R² values. This shows that the closed-form solution is correctly implemented and generalizes just like the package model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958db42-d3bc-4747-99f4-b83eaa1cf00b",
   "metadata": {},
   "source": [
    "#### Problem #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8997d7bc-407d-483b-adcd-d6347a4633d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Degree     Train MSE  Train R^2      Test MSE  Test R^2\n",
      "0       1  66319.347785   0.492384  76484.977062  0.494069\n",
      "1       2  58871.855127   0.549388  82113.931184  0.456835\n",
      "2       3  58862.529017   0.549459  83663.526745  0.446585\n",
      "3       4  58818.304351   0.549798  88922.167903  0.411800\n",
      "4       5  58812.805203   0.549840  85501.107850  0.434429\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Select only the sqft_living feature\n",
    "X_single = X_scaled[[\"sqft_living\"]].values\n",
    "\n",
    "# Function to create polynomial features up to degree p\n",
    "def polynomial_features(X, degree):\n",
    "    \"\"\"\n",
    "    X: original feature column (N x 1)\n",
    "    degree: maximum degree of polynomial\n",
    "    Returns: N x degree array of polynomial features (including X^1 to X^degree)\n",
    "    \"\"\"\n",
    "    return np.hstack([X**i for i in range(1, degree+1)])\n",
    "\n",
    "# Function to fit polynomial regression using closed-form solution\n",
    "def fit_polynomial_regression(X, y):\n",
    "    # Add bias column\n",
    "    X_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "    # Compute weights using pseudo-inverse for stability\n",
    "    w = np.linalg.pinv(X_bias) @ y\n",
    "    return w\n",
    "\n",
    "# Function to predict with polynomial regression\n",
    "def predict_polynomial(X, w):\n",
    "    X_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "    return X_bias @ w\n",
    "\n",
    "# Split sqft_living into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_single, X_test_single, y_train_single, y_test_single = train_test_split(\n",
    "    X_single, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Degrees to try\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for p in degrees:\n",
    "    # Generate polynomial features\n",
    "    X_train_poly = polynomial_features(X_train_single, p)\n",
    "    X_test_poly = polynomial_features(X_test_single, p)\n",
    "    \n",
    "    # Fit model\n",
    "    w_poly = fit_polynomial_regression(X_train_poly, y_train_single)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = predict_polynomial(X_train_poly, w_poly)\n",
    "    y_test_pred = predict_polynomial(X_test_poly, w_poly)\n",
    "    \n",
    "    # Compute metrics\n",
    "    train_mse = mean_squared_error(y_train_single, y_train_pred)\n",
    "    train_r2 = r2_score(y_train_single, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test_single, y_test_pred)\n",
    "    test_r2 = r2_score(y_test_single, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Degree\": p,\n",
    "        \"Train MSE\": train_mse,\n",
    "        \"Train R^2\": train_r2,\n",
    "        \"Test MSE\": test_mse,\n",
    "        \"Test R^2\": test_r2\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4105e-9955-457e-92e7-3c4ece6a36ec",
   "metadata": {},
   "source": [
    "As the degree of the polynomial increases from 1 to 5, the training MSE decreases slightly and the training R² increases, showing that higher-degree polynomials fit the training data better. However, the testing MSE increases and the testing R² decreases for higher-degree models, which indicates that the model starts to overfit the training data and does not generalize as well. This demonstrates the tradeoff between bias and variance: higher-degree polynomials reduce bias but can increase variance and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f024185-83a2-4185-847c-2b52d294d3a7",
   "metadata": {},
   "source": [
    "#### Problem #5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f38a153e-4646-4f46-a33f-b4c6dc42c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.01, iterations=100):\n",
    "    \"\"\"\n",
    "    X: feature matrix with bias term already added (N x d+1)\n",
    "    y: target vector (N x 1)\n",
    "    alpha: learning rate\n",
    "    iterations: number of iterations\n",
    "    Returns:\n",
    "        theta: learned parameters (d+1 x 1)\n",
    "        history: list of MSE at each iteration (optional)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  \n",
    "    d = X.shape[1]  \n",
    "    theta = np.zeros(d)  \n",
    "    history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_pred = X @ theta\n",
    "        gradient = (1/m) * X.T @ (y_pred - y)\n",
    "        theta -= alpha * gradient\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        history.append(mse)\n",
    "\n",
    "    return theta, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa94f2-c264-43c6-bf35-2b08c156ef1c",
   "metadata": {},
   "source": [
    "#### Problem #5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ba7979d-1271-47d4-bf5c-efecbac12a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Learning Rate  Iterations     Train MSE     Train R^2      Test MSE  \\\n",
      "0           0.01          10  3.237865e+05 -1.478300e+00  3.474337e+05   \n",
      "1           0.01          50  1.563500e+05 -1.967213e-01  1.684999e+05   \n",
      "2           0.01         100  8.256856e+04  3.680104e-01  9.157093e+04   \n",
      "3           0.10          10  7.865950e+04  3.979308e-01  8.745376e+04   \n",
      "4           0.10          50  4.000497e+04  6.937972e-01  4.620990e+04   \n",
      "5           0.10         100  3.984818e+04  6.949973e-01  4.599980e+04   \n",
      "6           0.50          10  2.213264e+08 -1.693058e+03  2.390183e+08   \n",
      "7           0.50          50  7.336029e+22 -5.615084e+17  7.919689e+22   \n",
      "8           0.50         100  1.037753e+41 -7.943084e+35  1.120317e+41   \n",
      "\n",
      "       Test R^2                                    Theta (first 5)  \n",
      "0 -1.298196e+00  [51.44113132296274, 8.336815833047464, 15.0470...  \n",
      "1 -1.145887e-01  [212.73583650944065, 12.644058759000448, 33.25...  \n",
      "2  3.942788e-01  [341.6623888974013, 3.796572154590444, 33.8630...  \n",
      "3  4.215130e-01  [351.01702513439955, 3.4777774607735923, 33.93...  \n",
      "4  6.943319e-01  [536.6591591142106, -26.04546653567061, 31.710...  \n",
      "5  6.957217e-01  [539.4849984874539, -29.9348592067174, 33.8879...  \n",
      "6 -1.580052e+03  [549.9879864935931, -1697.6388399043344, -2440...  \n",
      "7 -5.238696e+17  [201421119.3924234, -30419148337.91695, -45014...  \n",
      "8 -7.410647e+35  [2.395632987388635e+17, -3.6179577665052226e+1...  \n"
     ]
    }
   ],
   "source": [
    "# Add bias term\n",
    "X_train_bias = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_bias = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "# Learning rates and iteration counts to test\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "iterations_list = [10, 50, 100]\n",
    "\n",
    "# Store results\n",
    "results_gd = []\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    for iters in iterations_list:\n",
    "        # Train gradient descent\n",
    "        theta, history = gradient_descent(X_train_bias, y_train, alpha=alpha, iterations=iters)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = X_train_bias @ theta\n",
    "        y_test_pred = X_test_bias @ theta\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Save results\n",
    "        results_gd.append({\n",
    "            \"Learning Rate\": alpha,\n",
    "            \"Iterations\": iters,\n",
    "            \"Train MSE\": train_mse,\n",
    "            \"Train R^2\": train_r2,\n",
    "            \"Test MSE\": test_mse,\n",
    "            \"Test R^2\": test_r2,\n",
    "            \"Theta (first 5)\": theta[:5]  \n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "import pandas as pd\n",
    "results_gd_df = pd.DataFrame(results_gd)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(results_gd_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a8cfe-6e69-46f2-832a-c56b94ec056a",
   "metadata": {},
   "source": [
    "#### Problem #5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24673f2-0a97-4f32-9c0c-e6204a52d91d",
   "metadata": {},
   "source": [
    "From the results, we can see that the learning rate has a big impact on the behavior of gradient descent. A small learning rate (0.01) converges slowly: MSE decreases and R² gradually improves as iterations increase. A moderate learning rate (0.1) reaches good accuracy quickly, with training and testing MSE close to the closed-form solution after 50–100 iterations. A large learning rate (0.5) is too high, causing the algorithm to diverge: MSE and R² explode to extremely large or negative values, and the algorithm fails to converge. Overall, the number of iterations needed depends on the learning rate, and for a reasonable learning rate (like 0.1), the algorithm converges to a solution very close to the optimal closed-form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a6848-2ca3-4e6c-b050-7f58c37eb229",
   "metadata": {},
   "source": [
    "#### Problem #6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1b1b066-b108-46b6-9216-98a5392f0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_ridge(X, y, alpha=0.01, iterations=100, lam=1.0):\n",
    "    \"\"\"\n",
    "    Gradient descent for ridge regression.\n",
    "    X: feature matrix with bias\n",
    "    y: target vector\n",
    "    alpha: learning rate\n",
    "    iterations: number of iterations\n",
    "    lam: regularization parameter\n",
    "    Returns:\n",
    "        theta: learned parameters\n",
    "        history: list of MSE per iteration\n",
    "    \"\"\"\n",
    "    m, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "    history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_pred = X @ theta\n",
    "        gradient = (1/m) * X.T @ (y_pred - y)\n",
    "        # regularize all except bias term\n",
    "        reg_term = (lam/m) * np.r_[0, theta[1:]]\n",
    "        theta -= alpha * (gradient + reg_term)\n",
    "        history.append(mean_squared_error(y, y_pred))\n",
    "    return theta, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5274058-e7e6-4645-989c-d475804e7e0c",
   "metadata": {},
   "source": [
    "#### Problem #6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5eee404c-d2e2-465c-8822-4320438d20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "Slope: 1.922607418472327 Intercept: 1.194775353930857\n",
      "MSE: 3.899871527117831 R^2: 0.5638856153211096\n",
      "\n",
      "Ridge Regression (lambda=1):\n",
      "Slope: 1.9211990616932049 Intercept: 1.1947204649341232\n",
      "MSE: 3.8998742328515537 R^2: 0.5638853127446299\n",
      "\n",
      "Ridge Regression (lambda=10):\n",
      "Slope: 1.908616091571829 Intercept: 1.194230058940315\n",
      "MSE: 3.9001385678344787 R^2: 0.563855752722556\n",
      "\n",
      "Ridge Regression (lambda=100):\n",
      "Slope: 1.7912945390435633 Intercept: 1.1896575937744818\n",
      "MSE: 3.923393531500514 R^2: 0.5612551993198549\n",
      "\n",
      "Ridge Regression (lambda=1000):\n",
      "Slope: 1.109370662324776 Intercept: 1.1630804380443334\n",
      "MSE: 4.802052525912093 R^2: 0.46299662233191596\n",
      "\n",
      "Ridge Regression (lambda=10000):\n",
      "Slope: 0.2307882146811241 Intercept: 1.1288387531144035\n",
      "MSE: 7.804390863179958 R^2: 0.12725147599805908\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Simulate N=1000 samples\n",
    "N = 1000\n",
    "np.random.seed(42)\n",
    "X_sim = np.random.uniform(-2, 2, N).reshape(-1, 1)\n",
    "e = np.random.normal(0, 2, N)\n",
    "y_sim = 1 + 2*X_sim.flatten() + e\n",
    "\n",
    "# Add bias column\n",
    "X_sim_bias = np.c_[np.ones(N), X_sim]\n",
    "\n",
    "# Linear regression closed-form\n",
    "theta_lin = np.linalg.pinv(X_sim_bias) @ y_sim\n",
    "y_pred_lin = X_sim_bias @ theta_lin\n",
    "mse_lin = mean_squared_error(y_sim, y_pred_lin)\n",
    "r2_lin = r2_score(y_sim, y_pred_lin)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Slope:\", theta_lin[1], \"Intercept:\", theta_lin[0])\n",
    "print(\"MSE:\", mse_lin, \"R^2:\", r2_lin)\n",
    "\n",
    "# Ridge regression for different lambda values\n",
    "lambdas = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for lam in lambdas:\n",
    "    # Closed-form ridge regression\n",
    "    I = np.eye(X_sim_bias.shape[1])\n",
    "    I[0,0] = 0  # do not regularize bias\n",
    "    theta_ridge = np.linalg.pinv(X_sim_bias.T @ X_sim_bias + lam*I) @ X_sim_bias.T @ y_sim\n",
    "    y_pred_ridge = X_sim_bias @ theta_ridge\n",
    "    mse_ridge = mean_squared_error(y_sim, y_pred_ridge)\n",
    "    r2_ridge = r2_score(y_sim, y_pred_ridge)\n",
    "    \n",
    "    print(f\"\\nRidge Regression (lambda={lam}):\")\n",
    "    print(\"Slope:\", theta_ridge[1], \"Intercept:\", theta_ridge[0])\n",
    "    print(\"MSE:\", mse_ridge, \"R^2:\", r2_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268c9b5-abe9-41f8-bbda-3b4a60e8ce74",
   "metadata": {},
   "source": [
    "As the regularization parameter increases, the slope of the fitted line decreases toward zero, and the model coefficients shrinks more strongly. Small values of lambda (1–10) give results very similar to ordinary linear regression, while larger values (100–10000) reduce the slope substantially, increase MSE, and lower R². This shows that strong regularization reduces variance but increases bias, making the model simpler and less sensitive to the data, but also less accurate for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaba9f3-7578-4c02-ae1c-9981fb6a233e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
